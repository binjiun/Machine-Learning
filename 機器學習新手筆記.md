![](https://i.imgur.com/NGYDjwQ.png)
![](https://i.imgur.com/bjpGCUr.jpg)
https://www.itread01.com/p/517698.html


# 機器學習
1. 機器學習理論主要是設計和分析一些讓電腦可以自動「學習」的演算法。

2. 機器學習演算法是一類從資料中自動分析獲得規律，並利用規律對未知資料進行預測的演算法。

---
比賽 Kaggle
## 機器學習種類

1. 監督式學習 (supervised learning)

    S. B. Kotsiantis 的論文是這樣定義的:

    英文:
    Supervised machine learning is the search for algorithms that reason from externally
    To produce general hypotheses, which then make predictions about future instances.
    Goal of supervised learning is to build a concise model of the distribution of class labels in terms of Predictor features.

    中文翻譯:
    監督式機器學習是搜索來自外部的演算法產生一般假設，然後對未來的實例進行預測。監督學習的目標是建立一個簡潔的模型來分配類別標籤(分群)進行預測特徵值。
    
    
  
    監督式學習是從已經有答案的「訓練數據集(traininf dataset)」來辨別出不
    同的數據的特徵情況，找出模型，這個模型可以讓未知的數據中，找出特徵。

    訓練數據集(traininf dataset)：
    • 數據內容
    • 結果稱為「分類」Labels
    透過演算法，當有新的資料的時候，就能透過模型來進行預測，找出我們所想要的特徵值。
    常見的演算法一般常見的分類如下：
    • 二元分類法(Binary classification)：只有兩種答案，例如是或不是。
    • 多元分類法(multi-class classification)：答案有多種以上，例如：依照判斷的影像判斷，答案是車子、機車、腳踏車、飛機。
    • 迴歸分析(Regression Analysis)：是一種統計學上分析數據的方法，目的在於了解兩個    或多個變數間是否相關、相關方向與強度，並建立數學模型以便觀察特定變數來預測研究者感興趣的變數。[最早出現在Francis Galton的身高迴歸 子女儘管有遺傳因子 但經過數代之後仍然會迴歸平均值](https://projecteuclid.org/download/pdf_1/euclid.ss/1177012581)


    論文依據:[
Supervised Machine Learning: A Review of Classification Techniques , Informatica Journal 31 (2007) 249-268](https://datajobs.com/data-science-repo/Supervised-Learning-[SB-Kotsiantis].pdf)
    
---

2. 非監督式的學習　(unsupervised learning)

    S. B. Kotsiantis 的論文是這樣定義的:
    
    英文:
     Unsupervised learning (clustering) deals with instances, which have not     been pre-classified in any
    way and so do not have a class attribute associated with them. The scope     of applying clustering algorithms is to
    discover useful but unknown classes of items. Unsupervised learning is an     approach of learning where instances
    are automatically placed into meaningful groups based on their         similarity. 
    
    中文翻譯:
    非監督學習 (聚類) 處理的實例, 並沒有預先歸類在任何方式, 所以沒有與類屬性關聯的。
    聚類演算法的範圍是 發現有用但未知的分類類別。非監督學習是學習的一種方法。 根據它們的       相似性自動放置到有意義的組中。


    非監督式的學習 (unsupervised　learning)通常用在不知道預測的答案，然後把手上
    所有資料，依照分類，後從中間去找一個相似度最高的。
    
    論文依據:
    [S. Kotsiantis, P. Pintelas, Recent Advances in Clustering: A Brief Survey, WSEAS Transactions on Information Science and Applications, Vol 1, No 1 (73-81), 2004.](https://www.researchgate.net/publication/228084514_Recent_advances_in_clustering_A_brief_survey)
    
---
3. 強化學習(reinforcement learning)

     Leslie Pack Kaelbling 的論文指出:
     
     英文:
     Its promise is beguiling|a way of programming agents by reward and             punishment without
    needing to specify how the task is to be achieved.
     
     中文翻譯(暫定):
     
     它的承諾是一種通過獎勵和懲罰來讓機器編程的方式
     撰寫人需要指定如何實現任務。
     
    強化式學習的原理是經由不同的動作狀態和獎勵的方法，不斷的訓練機器依照目標
    邁進。
    
    論文依據:
    [Reinforcement Learning: A Survey (1996) [LP Kaelbling, ML Littman, AW Moore] [49pp]](https://arxiv.org/pdf/cs/9605103.pdf)
    
---

## 初學者十大演算法

五個監督學習演算法：線性迴歸、邏輯迴歸、CART、樸素貝葉斯和 KNN

三個無監督學習演算法：Apriori、K-means 和主成分分析 

兩個整合學習技法：以隨機森林來套袋和以 AdaBoost 來作提升

原文網址：https://itw01.com/Q9KVEXB.html

## 線性迴歸(Linear regression)
線性迴歸可以被用來預測連續數值，例如：房價預測、股票預測等。
但想要解決的問題必需是線性的。

1. 線性(Linear)
   從數學上來講，是指方程的解滿足線性疊加原理，即方程任意兩個解的線性疊加仍然是方程的一個解。

    (1)沒有變數擁有 1 以上的指數
    (2)沒有變數在絕對值中 
    (3)沒有變數在符號函數(Sign function)中
    
    ![](https://i.imgur.com/kkhBinz.png)
    
    [參考資料](https://medium.com/@prairie5270/learn-linear-regression-with-tensorflow-f9c2fcc76214)
    
2. 迴歸(Regression)
    迴歸是一種統計學上分析數據的方法，其目的在於找出一個最能夠代表觀測資料關係的函數，它所得出的結論是連續的。
    
    在只有兩個變數的平面上，也就是二維空間，找出一條能夠代表資料的線，我們稱之為簡單線性回歸(Simple Linear Regression)。

    三維以上的空間中，利用線性回歸找出一能夠代表資料的超平面(Hyperplane)，我們稱之為多元線性回歸 (Multiple Linear Regression)。


3. 用Python實作線性迴歸

```
import pandas as pd
from sklearn import linear_model
import matplotlib.pyplot as plt

#read data
dataframe = pd.read_fwf('brain_body.txt')
x_values = dataframe[['Brain']]
y_values = dataframe[['Body']]

#train model on data
body_reg = linear_model.LinearRegression()
body_reg.fit(x_values, y_values)

#visualize results
plt.scatter(x_values, y_values)
plt.plot(x_values, body_reg.predict(x_values))
plt.show()
```

## 非線性迴歸
非線性迴歸中很大一部分是基于多项式的回歸模型，即利用曲線方程代替直線方程擬合座標圖上各點，使得各點到曲線的距離總和最短。 
  一元m次多项式迴歸方程為：
  ![](https://i.imgur.com/pMVyobB.png)

  二元二次多项式迴歸方程為：
![](https://i.imgur.com/ilxKbxW.png) 

  多项式回归的最大优点就是可以通过增加x的高次项对实测点进行逼近，直至满意为止。事实上，多项式回归可以处理很大一类非线性问题，它在分析中占有重要的地位，因为任何一个函数都可以分段用多项式来逼近。 
  [參考資料](https://carrylaw.github.io/bml/2017/10/13/ml02/)
## K-mean

演算法優缺點： 優點：容易實現
缺點：可能收斂到區域性最小值，在大規模資料集上收斂較慢 
使用資料型別：數值型資料 

演算法思想 k-means演算法實際上就是通過計算不同樣本間的距離來判斷他們的相近關係的，相近的就會放到同一個類別中去。 1.首先我們需要選擇一個k值，也就是我們希望把資料分成多少類，這裏k值的選擇對結果的影響很大，Ng的課說的選擇方法有兩種一種是elbow method，簡單的說就是根據聚類的結果和k的函式關係判斷k為多少的時候效果最好。另一種則是根據具體的需求確定，比如說進行襯衫尺寸的聚類你可能就會考慮分成三類（L,M,S）等 


2.然後我們需要選擇最初的聚類點（或者叫質心），這裏的選擇一般是隨機選擇的，程式碼中的是在資料範圍內隨機選擇，另一種是隨機選擇資料中的點。這些點的選擇會很大程度上影響到最終的結果，。這裏有兩種處理方法，一種是多次取均值，另一種則是後面的改進演算法（bisecting K-means）
 3.終於我們開始進入正題了，接下來我們會把資料集中所有的點都計算下與這些質心的距離，把它們分到離它們質心最近的那一類中去。完成後我們則需要將每個簇算出平均值，用這個點作為新的質心。反覆重複這兩步，直到收斂我們就得到了最終的結果。 
 
 
[參考資料](https://itw01.com/V9RCET5.html)

```
1.載入套件
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

2.匯入scikit learn中的make_blobs資料集

from sklearn.datasets import make_blobs

#建立資料，選擇樣本數、變數、群組、群組標準差、隨機種子
data = make_blobs(n_samples=200,n_features=2,centers=4,cluster_std=1.8,random_state=101)


3.視覺化資料

#在data[0]中，可以看到這是一個二維陣列
#[:,0]=[第一橫排:最後橫排 ,第0行（或稱第一個變數）]
#[:,1]=[第一橫排:最後橫排 ,第1行（或稱第二個變數）]
#data[1]代表分群結果
plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')
4.載入KMeans建立Clusters（群組）

from sklearn.cluster import KMeans

#設定分成四組（因為知道原始資料是分成四組，但改成其他分組數字，就可以從下方圖表比較出差異）
kmeans = KMeans(n_clusters=4)

#使用KMeans演算法來建立模型
kmeans.fit(data[0])

#因為分成四組，所以centroid的點也有四個
kmeans.cluster_centers_

#分群結果
kmeans.labels_
5.製圖比較使用KMeans及原始分群結果

#plt.subplot(橫排數量,每個橫排有幾個圖並列)
#sharey=True共享y軸
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))
ax1.set_title('K Means')
ax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')
ax2.set_title("Original")
ax2.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')
```
## 決策樹

[參考資料](https://carrylaw.github.io/bml/2017/10/13/ml02/)
[決策樹參考資料](https://itw01.com/Q2XPE2K.html)
[決策樹](https://pythonfun.top/decision-trees-with-scikit-learn-tsb/)

## 隨機森林

[隨機森林](https://ljalphabeta.gitbooks.io/python-/content/randomforest.html)

## 貝式分類

[貝式分類理論基礎](http://mropengate.blogspot.com/2015/06/ai-ch14-3-naive-bayes-classifier.html)

## KNN最近鄰居法
[KNN最近鄰居法]tw01.com/4R73EXI.html)

overfiting and underfiting
===

如果訓練誤差總是無法降低，預測的準確率很低，我們會稱這種情況為欠擬合 (underfitting)；而如果訓練誤差很低時，在訓練資料的表現很好，但是卻在在測試集上無法獲得較好的結果，則會稱這種情況為過擬合 (overfitting)。

函數
===

線性 linear
梯度 gradient
 乙狀(Sigmoid Function) 
指數 index
多項試 
Polynomial
smooth

梯度
===
https://blog.csdn.net/walilk/article/details/50978864

梯度下降
最佳化

降維方法
===
[七種降維方法](https://t17.techbang.com/topics/37183-data-analysis-in-the-field-of-seven-of-the-best-dimension-reduction-method?mode=print&page=1)

[四種降維方法](https://read01.com/zh-tw/nx7QEo.html#.W_JAgjgzaUk)
## 未整理參考資料
指數平滑
傅立葉逼近(線性代數)
[建議機器學習論文書單](https://tw.saowen.com/a/bfe61f5eb6b58a1ce5c6549fdcfc5552c7f611a468d1af51b060c787d3820720)
[機器學習演算法架構圖](https://codertw.com/%E7%A8%8B%E5%BC%8F%E8%AA%9E%E8%A8%80/452756/
)
[GITHUB機器學習頂級論文](http://toments.com/111819/)
[學習地圖](https://medium.com/@allenyllee/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%9C%B0%E5%9C%96-9bbddf0e9eb3)
[十大演算法](https://wizardforcel.gitbooks.io/dm-algo-top10/content/k-means.html)
[七種迴歸](https://read01.com/zh-tw/mAkMnd.html#.W-wc6TgzaUl)
[大數據系統學習法](https://buzzorange.com/techorange/2017/06/20/start-from-zero-learning-big-data/)

林軒田 機器學習基石
莫煩python


the coding train

machine learning tv


[mechine learning crash course](https://developers.google.com/machine-learning/crash-course/)
[演算法種類](https://docs.microsoft.com/zh-tw/azure/machine-learning/studio/algorithm-choice)
[演算法ㄧ](https://brohrer.mcknote.com/zh-Hㄧㄢant/using_machine_learning/find_the_right_algorithm.html)
[交叉驗證](https://medium.com/@chih.sheng.huang821/%E4%BA%A4%E5%8F%89%E9%A9%97%E8%AD%89-cross-validation-cv-3b2c714b18db)

[2018](http://wiki.swarma.net/index.php?title=%E9%A1%B6%E7%BA%A7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E7%9A%84%E9%A1%B9%E7%9B%AE%E5%AE%9E%E8%B7%B5%EF%BC%9A%E5%B9%B4%E5%BA%A6_Top50&variant=zh-hk)

頂級論文
https://zhuanlan.zhihu.com/p/30918081

排成演算法
https://publish.get.com.tw/BookPre_pdf/51MG060707-2.pdf

https://zhuanlan.zhihu.com/p/48914251

ROC
https://en.wikipedia.org/wiki/Receiver_operating_characteristic

離傘數學

https://www.slideshare.net/ccckmit/ss-57362287