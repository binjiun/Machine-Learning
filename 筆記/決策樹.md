# 決策樹(Decision Tree)


## 簡介
功能:Classification、regression、補值、特徵篩選

1.分類樹(Classification Tree):

    當其用來預測Y是類別型態(例如：生或死、男或女)時，該決策樹便稱為分類樹

2.迴歸樹(Regression Tree):

    當其用來預測Y是連續型態(例如：身高、體重)時，該決策樹便稱為迴歸樹
    
3.分類迴歸樹(Classification and Regression Tree，簡稱CART):

    結合分類樹與迴歸樹的特性，其預測結果不僅可以呈現類別型態，也可以是數值型的資料。
    


## 原理

二元決策樹中常使用三種 impurity(不純度) measures:
Entropy , Gini , Classification Error

變數說明
Pj: Class j 的機率

1. Entropy (a way to measure impurity):
    ![](https://i.imgur.com/sUcxd4z.png)

如果節點的所有樣本都屬於同一類，則Entropy 為0；如果樣本具有一樣的分佈，則Entropy為最大。

換句話說，節點的Entropy（由單個類別組成）為零，因為概率為1，log（1）=0。

當節點中的所有類別都具有相同的概率時，Entropy達到最大值。

![](https://i.imgur.com/HGjpQ0K.png)


### 檢測資料Y(類別)的Entropy(資料雜亂程度)


    entropy =−1 * log2(1)= 0

此時樣本只具有同種類別，並不是很好的traing 環境。
因為只猜0 即可達到100 % 準確率
這種狀況叫做(不平橫資料) 請查看GITHUB另一篇文章

    entropy = −0.5 * log2(0.5) − 0.5 * log2(0.5) = 1
此時為很好的training資料集。 因為各占一半的比率。

2. Gini index (a criterion to minimize the probability of misclassification):

   ![](https://i.imgur.com/Jvvxp8d.png)

GINI 也用在檢測不純度的大小

    Gini=1 −(p1 ^ 2 + p2 ^ 2) =1−(0.5 ^ 2+0.5 ^ 2)=0.5
 
當兩類別的機率相等時，GINI為最大，純度最小。反之則最小，純度最大。

---
3. Classification Error:
     ![](https://i.imgur.com/syxgUvH.png)


## Information Gain(資料獲利):

使用決策樹算法，我們從樹的root開始，然後在特徵上分割數據，從而獲得最大的資料獲利（IG）。

決策樹在每個子節點上**重複**此拆分過程，直到空著的葉子。這意味著每個節點上的樣本都屬於同一類。(直到不純度最小)


 
但是，這可能會導致具有許多節點的非常深(Depth)的樹，這很容易導致過度擬合(Overfitting)。因此，我們通常希望通過設置決策樹的最大深度的限制來修剪樹。

基本上，使用IG，我們想確定給定訓練特徵向量集中的哪個屬性最有用。換句話說，IG告訴我們特徵向量的給定屬性有多重要。

* 我們將使用IG來確定決策樹 節點中屬性的順序。

## Gain Rato(獲利率)
假設某個資料有index，我只需要按照index分割即可得到最大的IG(資料獲利)，但這樣是無意義的分割，且會造成過度擬和(OVERFITTING)，深度過深等問題，因此搭配逞罰項(SplitINFo)。

N:為總數 
Ni : Class i  的 個數
![](https://i.imgur.com/FxESHep.png)


## 流程

1. 資料設定：將原始資料分成兩組，一部
分為訓練資料，一部分為測試資料

2. 決策樹生成：使用訓練資料來建立決策
樹，而在每一個內部節點，則依據屬性
選擇指標 (如：資訊理論(Information
Theory)…) 來評估選擇哪個屬性做分支
的依據。又稱節點分割 (Splitting Node)
3. 剪枝：使用測試資料來進行決策樹修剪
4. 
![](https://i.imgur.com/EOyjrtx.png)

> 引用國立聯合大學 教學PTT
## python 程式碼
```
import matplotlib.pyplot as plt
import numpy as np

def gini(p):
   return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))

def entropy(p):
   return - p*np.log2(p) - (1 - p)*np.log2((1 - p))

def classification_error(p):
   return 1 - np.max([p, 1 - p])

x = np.arange(0.0, 1.0, 0.01)
ent = [entropy(p) if p != 0 else None for p in x]
scaled_ent = [e*0.5 if e else None for e in ent]
c_err = [classification_error(i) for i in x]

fig = plt.figure()
ax = plt.subplot(111)

for j, lab, ls, c, in zip(
      [ent, scaled_ent, gini(x), c_err],
      ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'],
      ['-', '-', '--', '-.'],
      ['lightgray', 'red', 'green', 'blue']):
   line = ax.plot(x, j, label=lab, linestyle=ls, lw=1, color=c)

ax.legend(loc='upper left', bbox_to_anchor=(0.01, 0.85),
         ncol=1, fancybox=True, shadow=False)

ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')
ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')

plt.ylim([0, 1.1])
plt.xlabel('p(j=1)')
plt.ylabel('Impurity Index')
plt.show()
```

## 停止條件
決策樹的過程是一個遞迴方程式，因此需要加上停止條件才能使過程停止。停止條件可以如下
1. 當RMSE進步小於0.001
2. 深度N層停止

以上都是為了減少OVERfitting的產生機率，但除了訓練過度外還有以下幾種可能。
1. 缺少高度相關資料:訓練資料並沒有包含關鍵變數，導致結果不理想，可以看ROC或是混淆矩陣（Confusion Matrix）來分析結果。
2. 多重比較（Mulitple Comparision）:太多相關性低的資料干擾結果。
## 優化方案
1. Prune Tree(裁減樹枝):
*  提前停止(earlystop):在構建的過程中，偵測各種條件可以提前收斂。
*  後置裁減:決策樹構建好後才開始裁減。1）用單一葉節點代替整個子樹，葉節點的分類採用子樹中最主要的分類；2）將一個子樹完全替代另外一顆子樹。
2. K-Fold Cross Validation(交叉驗證):資料隨機平均分成k個集合，然後將某一個集合當做「測試資料(Testing data)」，剩下的k-1個集合做為「訓練資料(Training data)」，如此重複進行直到每一個集合都被當做「測試資料(Testing data)」為止。最後的結果(Predication results)在和真實答案(ground truth)進行成效比對(Performance Comparison)。
3. Random Forest(隨機森林):Random Forest是用訓練資料隨機的計算出許多決策樹，形成了一個森林。然後用這個森林對未知資料進行預測，選取投票最多的分類。
## 參考及引用資料

1. [原理教程](https://www.bogotobogo.com/python/scikit-learn/scikt_machine_learning_Decision_Tree_Learning_Informatioin_Gain_IG_Impurity_Entropy_Gini_Classification_Error.php)

2. [AI - Ch14 機器學習(2), 決策樹 Decision Tree](https://mropengate.blogspot.com/2015/06/ai-ch13-2-decision-tree.html)

3. [聯合大學 機器學習 ](http://debussy.im.nuu.edu.tw/sjchen/MachineLearning/final/CLS_DT.pdf)

4. [決策樹](https://www.slideshare.net/XavierYin/ss-62151265)