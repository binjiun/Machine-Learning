# 隨機森林 and Ensemble learning

## 前言 整體學習(Ensemble learning)

前提:每個分類器之間應該要有差異，每個分類器準確率需大於0.5。

概念: 大眾的智慧

說明:如果用的分類器沒有差異，那只是用很多個一樣的分類器來分類，結果合成起來是沒有差異的，但如果是根據資料分群用同個演算法的分類去合併是可以增加準確度的(例如: 時間序列分成:早上、中午、下午各一個分類氣)。如果分類器的準確率p<0.5，隨著ensemble規模的增加，分類準確率不斷下降；如果準確率大於p>0.5，那麼最終分類準確率可以趨向於1。

Ensemble learning方法:
1. Bagging
2. Boosting
3. Stack 

---

### Bagging


原理: Bagging概念很簡單，從訓練資料中隨機抽取(**取出後放回**，n<N)樣本訓練多個分類器(要多少個分類器自己設定)，每個分類器的權重一致最後用投票方式(Majority vote)得到最終結果，而這種抽樣的方法在統計上稱為bootstrap。

![](https://i.imgur.com/2VX0JQo.png)


比較簡單的 model 會有 Bias 比較大，Variance 就比較小，
比較複雜的 model 會有 Bias 小 Variance 就比較大
兩者的組合下，error rate 隨著 model 複雜度增加逐漸下降，然後再逐漸上升 ( 因為 overfitting )，

把不同模型的輸出，通通集合起來，做一個平均，得到一個新的模型  y-hat 可能就會跟正確的模型很接近，
Bagging 就是要體現這件的事情！

### Boosting

BOOSTing方法: XGBOOST 、GradientBoost 、adaboost

先來介紹經典的ADABOOST,，舉個實際的例子：
平面上有 10 筆資料，要做二元分類

![](https://i.imgur.com/qChzMvM.png)
f1 分類器切下去，發現有 3 筆分類錯誤，所以錯誤率 epsilon_1 = 0.3
並可以由 epsilon 的值計算出 d1 = 1.53, alpha_1 = 0.42
接著，我們把分類錯的資料 weight 變大乘以 1.53 (d1)，分類對的 weight 變小 除以 1.53 (d1)


然後我們使用新的 classifier f2 切一刀，左邊是 positive 右邊是 negative，
針對我們上一步驟調整後的 data 來做分類
![](https://i.imgur.com/ukWPqr5.png)

f2 總共分類錯三筆 data，經過計算，可以得到 f2 的 error rate: epsilon_2  = 
(0.65 * 3) / (1.53 * 3 + 0.65 * 7) = 0.21 
再根據 epsilon_2 可以計算出 d2 = 1.94, alpha_2 = 0.66
然後我們把分類錯的資料 weight 變大乘以 1.94 (d2)，分類對的 weight 變小除以 1.94 (d2)
更新 data weight

第三個 classifier 切一刀，上面是 positive 下面是 negative，總共分錯三筆資料
![](https://i.imgur.com/7DkQZRs.png)
根據前面的方法，再次操作一遍，得到 epsilon_3 = 0.13, d3 = 2.59, alpha_3 = 0.95
假設我們只跑三個 iteration 就結束了，所以 weight 就打住不更新了


整合起來，得到最後的 Classifier H(x)
目前三個 classifier 把空間切成六塊
![](https://i.imgur.com/xS0pYNy.png)
左上角：大家都覺得是藍色的，沒有爭議就是藍的
右上角：f1 跟 f2 覺得是紅的 總共佔 0.42 + 0.66 = 1.08 ， f3 覺得是藍的但只有 0.95 輸了，
所以結論，右上角是紅的
左下角：f1, f2 都覺得是藍的，總共 weight 1.08 > f3 覺得是紅的且 weight 0.95 也輸了！
所以結論，左下角是藍的

f1, f2, f3這三個 classifier 分別都會犯錯，
但是我們透過 AdaBoost algorithm 把它們組合起來後，得到好的結果！

### Stack
Stack技巧 分兩種 : Voting ,stacking

![](https://i.imgur.com/fbhYPFI.png)


將各個分類器結合在一起，每個分類器佔有一票，最後按照票數多的選項進行輸出，但假設不是每一個系統都是好的(例如小明的 Classifier 根本亂寫)，但是他還是佔一票，這樣可能會把投票結果變糟。因此也有改進版，針對比較好的分類器給他較高的權重(例如:強的人佔有兩票)
![](https://i.imgur.com/R1k8dbH.png)
使用 Stacking 方法，把 training data 切成兩部分
一部分拿來 Learn 前面的 classifier，另一部分拿來 train 後面的 Final Classifier 。如果是迴歸就拿另一半的資料拿來訓練權重。

    stack 是Kaggle的常勝軍。
但他有很明顯的**缺點**，雖然可以增強一點點的預測效果，可是計算量是指數增加，因此在業界上要考慮到時間成本的問題。

##  隨機森林使用背景

    對 Decision Tree 做 Bagging 就是 Random Forest
###  隨機森林定義

隨機森林是一種比較新的機器學習模型。經典的機器學習模型是神經網路，有半個多世紀的歷史了。神經網路預測精確，但是計算量很大。上世紀八十年代Breiman等人發明分類樹的演算法（Breiman
et al. 1984），通過反覆二分資料進行分類或迴歸，計算量大大降低。2001年Breiman把分類樹組合成隨機森林（Breiman 2001a），即在變數（列）的使用和資料（行）的使用上進行隨機化，生成很多分類樹，再彙總分類樹的結果。隨機森林在運算量沒有顯著提高的前提下提高了預測精度。隨機森林對多元公線性不敏感，結果對缺失資料和非平衡的資料比較穩健，可以很好地預測多達幾千個解釋變數的作用（Breiman 2001b），被譽為當前最好的演算法之一（Iverson et al. 2008）。

隨機森林顧名思義，是用隨機的方式建立一個森林，森林裡面有很多的決策樹組成，隨機森林的每一棵決策樹之間是沒有關聯的。在得到森林之後，當有一個新的輸入樣本進入的時候，就讓森林中的每一棵決策樹分別進行一下判斷，看看這個樣本應該屬於哪一類（對於分類演算法），然後看看哪一類被選擇最多，就預測這個樣本為那一類。

###  隨機森林優點

隨機森林是一個最近比較火的演算法，它有很多的優點：

1. 在資料集上表現良好，兩個隨機性的引入，使得隨機森林不容易陷入過擬合

2. 在當前的很多資料集上，相對其他演算法有著很大的優勢，兩個隨機性的引入，使得隨機森林具有很好的抗噪聲能力

3. 它能夠處理很高維度（feature很多）的資料，並且不用做特徵選擇，對資料集的適應能力強：既能處理離散型資料，也能處理連續型資料，資料集無需規範化

4. 可生成一個Proximities=（pij）矩陣，用於度量樣本之間的相似性： pij=aij/N, aij表示樣本i和j出現在隨機森林中同一個葉子結點的次數，N隨機森林中樹的顆數

5. 在建立隨機森林的時候，對generlization error使用的是無偏估計

6. 訓練速度快，可以得到變數重要性排序（兩種：基於OOB誤分率的增加量和基於分裂時的GINI下降量

7. 在訓練過程中，能夠檢測到feature間的互相影響

8. 容易做成並行化方法(個人認為是很明顯的優點之一，現在電腦追求多核多執行續，很適合此類方法)

9. 實現比較簡單

### 隨機森林應用範圍

隨機森林主要應用於迴歸和分類。本文主要探討基於隨機森林的分類問題。隨機森林和使用決策樹作為基本分類器的（bagging）有些類似。以決策樹為基本模型的bagging在每次bootstrap放回抽樣之後，產生一棵決策樹，抽多少樣本就生成多少棵樹，在生成這些樹的時候沒有進行更多的干預。而隨機森林也是進行bootstrap抽樣，但它與bagging的區別是：在生成每棵樹的時候，每個節點變數都僅僅在隨機選出的少數變數中產生。因此，不但樣本是隨機的，連每個節點變數（Features）的產生都是隨機的。

許多研究表明， 組合分類器比單一分類器的分類效果好，隨機森林（random forest）是一種利用多個分類樹對資料進行判別與分類的方法，它在對資料進行分類的同時，還可以給出各個變數（基因）的重要性評分，評估各個變數在分類中所起的作用。

## 隨機森林方法理論介紹

###  隨機森林基本原理

隨機森林由LeoBreiman（2001）提出，它通過自助法（bootstrap）重取樣技術，從原始訓練樣本集N中有放回地重複隨機抽取k個樣本生成新的訓練樣本集合，然後根據自助樣本集生成k個分類樹組成隨機森林，新資料的分類結果按分類樹投票多少形成的分數而定。其實質是對決策樹演算法的一種改進，將多個決策樹合併在一起，每棵樹的建立依賴於一個獨立抽取的樣品，森林中的每棵樹具有相同的分佈，分類誤差取決於每一棵樹的分類能力和它們之間的相關性。特徵選擇採用隨機的方法去分裂每一個節點，然後比較不同情況下產生的誤差。能夠檢測到的內在估計誤差、分類能力和相關性決定選擇特徵的數目。單棵樹的分

類能力可能很小，但在隨機產生大量的決策樹後，一個測試樣品可以通過每一棵樹的分類結果經統計後選擇最可能的分類。


### 決策樹

決策樹（decision tree）是一個樹結構（可以是二叉樹或非二叉樹）。其每個非葉節點表示一個特徵屬性上的測試，每個分支代表這個特徵屬性在某個值域上的輸出，而每個葉節點存放一個類別。使用決策樹進行決策的過程就是從根節點開始，測試待分類項中相應的特徵屬性，並按照其值選擇輸出分支，直到到達葉子節點，將葉子節點存放的類別作為決策結果。

隨機森林是用隨機的方式建立一個森林，森林裡面有很多的決策樹組成，隨機森林的每一棵決策樹之間是沒有關聯的。在得到森林之後，當有一個新的輸入樣本進入的時候，就讓森林中的每一棵決策樹分別進行一下判斷，看看這個樣本應該屬於哪一類，然後看看哪一類被選擇最多，就預測這個樣本為那一類。

在建立每一棵決策樹的過程中，有兩點需要注意取樣與完全分裂。首先是兩個隨機取樣的過程，random forest對輸入的資料要進行行、列的取樣。對於行取樣，採用有放回的方式，也就是在取樣得到的樣本集合中，可能有重複的樣本。假設輸入樣本為N個，那麼取樣的樣本也為N個。這樣使得在訓練的時候，每一棵樹的輸入樣本都不是全部的樣本，使得相對不容易出現over-fitting。然後進行列取樣，從M個feature中，選擇m個（m << M）。之後就是對取樣之後的資料使用完全分裂的方式建立出決策樹，這樣決策樹的某一個葉子節點要麼是無法繼續分裂的，要麼裡面的所有樣本的都是指向的同一個分類。一般很多的決策樹演算法都一個重要的步驟——剪枝，但是這裡不這樣幹，由於之前的兩個隨機取樣的過程保證了隨機性，所以就算不剪枝，也不會出現over-fitting。


### 隨機森林模型的注意點

設有N個樣本，每個樣本有M個features，決策樹們其實都是隨機地接受n個樣本（對行隨機取樣）的m個feature（對列進行隨機取樣），每顆決策樹的m個feature相同。每顆決策樹其實都是對特定的資料進行學習歸納出分類方法，而隨機取樣可以保證有重複樣本被不同決策樹分類，這樣就可以對不同決策樹的分類能力做個評價。

## 參考及引用文章:
[機器學習: Ensemble learning之Bagging、Boosting和AdaBoost](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ensemble-learning%E4%B9%8Bbagging-boosting%E5%92%8Cadaboost-af031229ebc3)

[ML筆記](http://violin-tao.blogspot.com/2018/01/ml-ensemble.html)

[](http://violin-tao.blogspot.com/2018/01/ml-ensemble.html)